{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 7473\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 1319\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from openai import OpenAI\n",
    "import utils\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import utils\n",
    "\n",
    "# Set OpenAI client\n",
    "client = OpenAI(api_key=utils.get_OPENAI_API_KEY_DJ())\n",
    "\n",
    "# load gsm8k dataset\n",
    "gsm8k_dataset = load_dataset(\"gsm8k\", \"main\")\n",
    "gsm8k_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let S be the number of people on the first hundred years’ ship.\n",
      "The second hundred years’ ship had twice as many as the first, so it had 2S people.\n",
      "The third hundred years’ ship had twice as many as the second, so it had 2 * 2S = <<2*2=4>>4S people.\n",
      "All the ships had S + 2S + 4S = 7S = 847 people.\n",
      "Thus, the ship that the monster ate in the first hundred years had S = 847 / 7 = <<847/7=121>>121 people on it.\n",
      "#### 121\n"
     ]
    }
   ],
   "source": [
    "print(gsm8k_dataset[\"train\"][\"answer\"][10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checker Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(actual, expected):\n",
    "    \"\"\"\n",
    "    Checkes if the actual answer is correct or not\n",
    "\n",
    "    @param actual:      The actual answer\n",
    "    @param expected:    The expected answer\n",
    "    @return:            1 if the actual answer is correct, 0 otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "        Compare the actual answer with the expected answer. \n",
    "        If the actual answer is correct, respond with \"1\". \n",
    "        If the actual answer is incorrect, respond with \"0\". \n",
    "        Respond with a json object with \"answer\" and \"isCorrect\" fields.\n",
    "\n",
    "        Example Prompt:\n",
    "        Actual answer: The answer is 42.\n",
    "        Expected answer: 42\n",
    "\n",
    "        Let's think step by step. \n",
    "        The actual answer is 42. The expected answer is also 42. Thus, the actual answer is correct!\n",
    "\n",
    "        Response: {{\n",
    "            \"answer\": \"The answer is 42.\",\n",
    "            \"isCorrect\": 1\n",
    "        }}\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = \"\"\"\n",
    "        Actual answer: {actual}\n",
    "        Expected answer: {expected}\n",
    "    \"\"\"\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt.format(actual=actual, expected=expected.split(\"####\")[-1].strip()),\n",
    "            },\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo-0125\",\n",
    "        max_tokens=4096,\n",
    "        temperature=0,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "    result = chat_completion.choices[0].message.content\n",
    "    return ast.literal_eval(result)[\"isCorrect\"]\n",
    "\n",
    "def check_fast(actual, expected):\n",
    "    \"\"\"\n",
    "    Faster and simpler version of the check function.\n",
    "    \"\"\"\n",
    "    expected = expected.split(\"####\")[-1].strip()\n",
    "    return expected in actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check the check function\n",
    "# total = 0\n",
    "# max_tests = 10\n",
    "# for i in tqdm(range(max_tests)):\n",
    "#     res = check(gsm8k_dataset[\"train\"][\"answer\"][i], gsm8k_dataset[\"train\"][\"answer\"][i])\n",
    "#     if not res:\n",
    "#         print(gsm8k_dataset[\"train\"][\"answer\"][i])\n",
    "#     total += res\n",
    "\n",
    "# total/max_tests * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_llm(instruction_score_pairs, training_sample):\n",
    "    pairs_str = \"\"\n",
    "    for pair in instruction_score_pairs:\n",
    "        pairs_str += f\"input:\\n{pair[0]}\\nscore:\\n{pair[1]}\\n\"\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "        You are an optimization expert. The user has some texts along with their corresponding scores.\n",
    "        Your task is to generate a new piece of text in square brackets that scores as high as possible.\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = \"\"\"I have some texts along with their corresponding scores. The texts are arranged in ascending order\n",
    "        based on their scores, where higher scores indicate better quality.\n",
    "\n",
    "        {pairs_str}\n",
    "\n",
    "        The following exemplars show how to apply your text: you replace <INS> in each input with your\n",
    "        text, then read the input and give an output. We say your output is wrong if your output is different\n",
    "        from the given output, and we say your output is correct if they are the same.\n",
    "                \n",
    "        input:\n",
    "        Q: {q1}\n",
    "        A: <INS>\n",
    "        output:\n",
    "        {a1}\n",
    "\n",
    "        input:\n",
    "        Q: {q2}\n",
    "        A: <INS>\n",
    "        output:\n",
    "        {a2}\n",
    "\n",
    "        input:\n",
    "        Q: {q3}\n",
    "        A: <INS>\n",
    "        output:\n",
    "        {a3}\n",
    "        \n",
    "        Write your new text that is different from the old ones and has a score as high as possible. Write the\n",
    "        text in square brackets.\n",
    "    \"\"\"\n",
    "    q1, q2, q3 = training_sample[\"question\"][0], training_sample[\"question\"][1], training_sample[\"question\"][2]\n",
    "    a1, a2, a3 = training_sample[\"answer\"][0], training_sample[\"answer\"][1], training_sample[\"answer\"][2]\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt.format(pairs_str=pairs_str, q1=q1, q2=q2, q3=q3, a1=a1, a2=a2, a3=a3)},\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo-0125\",\n",
    "        max_tokens=4096,\n",
    "        temperature=1,\n",
    "    )\n",
    "    result = chat_completion.choices[0].message.content\n",
    "    return result[1:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer_lm(instruction, training_sample):\n",
    "    accuracy = 0\n",
    "    user_prompt = \"\"\"\n",
    "        Q: {question}\n",
    "        A: {instruction}\n",
    "    \"\"\"\n",
    "    for i in tqdm(range(len(training_sample))):\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_prompt.format(\n",
    "                        question=training_sample[\"question\"][i], instruction=instruction\n",
    "                    ),\n",
    "                },\n",
    "            ],\n",
    "            model=\"gpt-3.5-turbo-0125\",\n",
    "            max_tokens=4096,\n",
    "            temperature=0,\n",
    "        )\n",
    "        result = chat_completion.choices[0].message.content\n",
    "        accuracy += check_fast(result, training_sample[\"answer\"][i])\n",
    "\n",
    "    accuracy = accuracy / len(training_sample) * 100\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the Optimizer with the Scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [01:47<00:00,  4.15s/it]\n",
      "100%|██████████| 26/26 [01:35<00:00,  3.66s/it]\n",
      "100%|██████████| 26/26 [01:31<00:00,  3.51s/it]\n",
      "100%|██████████| 26/26 [01:21<00:00,  3.14s/it]\n",
      "100%|██████████| 26/26 [01:26<00:00,  3.34s/it]\n",
      "100%|██████████| 26/26 [01:27<00:00,  3.35s/it]\n",
      "100%|██████████| 26/26 [01:09<00:00,  2.69s/it]\n",
      "100%|██████████| 26/26 [01:16<00:00,  2.95s/it]\n",
      "100%|██████████| 26/26 [01:31<00:00,  3.53s/it]\n",
      "100%|██████████| 1/1 [11:31<00:00, 691.20s/it]\n"
     ]
    }
   ],
   "source": [
    "# Sample 3.5% of the training set\n",
    "INS_PER_STEP = 8\n",
    "EXEMPLARS_PER_STEP = 3\n",
    "MAX_INS_SCORE_PAIRS = 20\n",
    "SAMPLE_PERCENTAGE = 0.35/100\n",
    "alt_percentage = 0.05/100\n",
    "training_sample = gsm8k_dataset[\"train\"].shuffle(seed=42).select(range(int(len(gsm8k_dataset[\"train\"]) * SAMPLE_PERCENTAGE)))\n",
    "ins_score_pairs = {\"Let's think step by step to solve this problem.\": scorer_lm(\"Let's think step by step to solve this problem.\", training_sample)}\n",
    "\n",
    "steps = 1\n",
    "for i in tqdm(range(steps)):\n",
    "    # Optimizer LLM\n",
    "    exemplars = training_sample.shuffle()[:EXEMPLARS_PER_STEP]\n",
    "    instructions = [opt_llm(ins_score_pairs, exemplars) for _ in range(INS_PER_STEP)]\n",
    "    \n",
    "    # Scoring the new instructions\n",
    "    new_ins_score_pairs = {ins: scorer_lm(ins, training_sample) for ins in instructions}\n",
    "    ins_score_pairs.update(new_ins_score_pairs)\n",
    "    ins_score_pairs = dict(sorted(ins_score_pairs.items(), key=lambda x: x[1], reverse=True)[:MAX_INS_SCORE_PAIRS])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'To optimize profits when trading baseball cards, Nicky should aim to acquire cards that have a significantly higher value compared to the cards he trades. This strategic approach will ensure a larger profit margin, leading to greater financial gains in each transaction.': 92.3076923076923,\n",
       " 'According to the law of conservation of energy, the total energy within a closed system remains constant over time. This principle governs various natural phenomena and is fundamental to understanding the behavior of physical systems.': 92.3076923076923,\n",
       " \"Let's think step by step to solve this problem.\": 84.61538461538461,\n",
       " 'By analyzing the trading patterns, Nicky maximized his profit by leveraging the differing valuations of the baseball cards.': 84.61538461538461,\n",
       " 'To maximize your efficiency in sorting algorithms, implement quicksort with a time complexity of O(n log n) for optimal performance.': 84.61538461538461,\n",
       " 'To maximize profit in trading, always aim to exchange lower-value items for higher-value items. This strategy will ensure optimal gains and success in every trade. Be strategic, be smart, and always prioritize value over quantity.': 84.61538461538461,\n",
       " 'By conducting a comprehensive analysis of market trends and consumer behavior, companies can make informed decisions to optimize their business strategies and achieve sustainable growth.': 84.61538461538461,\n",
       " 'By analyzing past data trends and implementing advanced algorithms, we can optimize decision-making processes and enhance overall efficiency.': 80.76923076923077,\n",
       " 'To calculate the area of a circle, use the formula A = πr^2, where A represents the area and r is the radius of the circle. If the radius is 5 units, what is the area of the circle?': 34.61538461538461}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ins_score_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
