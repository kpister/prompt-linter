{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 7473\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 1319\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from openai import OpenAI\n",
    "import utils\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import utils\n",
    "\n",
    "# Set OpenAI client\n",
    "client = OpenAI(api_key=utils.get_OPENAI_API_KEY_DJ())\n",
    "\n",
    "# load gsm8k dataset\n",
    "gsm8k_dataset = load_dataset(\"gsm8k\", \"main\")\n",
    "gsm8k_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let S be the number of people on the first hundred years’ ship.\n",
      "The second hundred years’ ship had twice as many as the first, so it had 2S people.\n",
      "The third hundred years’ ship had twice as many as the second, so it had 2 * 2S = <<2*2=4>>4S people.\n",
      "All the ships had S + 2S + 4S = 7S = 847 people.\n",
      "Thus, the ship that the monster ate in the first hundred years had S = 847 / 7 = <<847/7=121>>121 people on it.\n",
      "#### 121\n"
     ]
    }
   ],
   "source": [
    "print(gsm8k_dataset[\"train\"][\"answer\"][10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checker Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(actual, expected):\n",
    "    \"\"\"\n",
    "    Checkes if the actual answer is correct or not\n",
    "\n",
    "    @param actual:      The actual answer\n",
    "    @param expected:    The expected answer\n",
    "    @return:            1 if the actual answer is correct, 0 otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "        Compare the actual answer with the expected answer. \n",
    "        If the actual answer is correct, respond with \"1\". \n",
    "        If the actual answer is incorrect, respond with \"0\". \n",
    "        Respond with a json object with \"answer\" and \"isCorrect\" fields.\n",
    "\n",
    "        Example Prompt:\n",
    "        Actual answer: The answer is 42.\n",
    "        Expected answer: 42\n",
    "\n",
    "        Let's think step by step. \n",
    "        The actual answer is 42. The expected answer is also 42. Thus, the actual answer is correct!\n",
    "\n",
    "        Response: {{\n",
    "            \"answer\": \"The answer is 42.\",\n",
    "            \"isCorrect\": 1\n",
    "        }}\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = \"\"\"\n",
    "        Actual answer: {actual}\n",
    "        Expected answer: {expected}\n",
    "    \"\"\"\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt.format(actual=actual, expected=expected.split(\"####\")[-1].strip()),\n",
    "            },\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo-0125\",\n",
    "        max_tokens=4096,\n",
    "        temperature=0,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "    result = chat_completion.choices[0].message.content\n",
    "    return ast.literal_eval(result)[\"isCorrect\"]\n",
    "\n",
    "def check_fast(actual, expected):\n",
    "    \"\"\"\n",
    "    Faster and simpler version of the check function.\n",
    "    \"\"\"\n",
    "    expected = expected.split(\"####\")[-1].strip()\n",
    "    return expected in actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check the check function\n",
    "# total = 0\n",
    "# max_tests = 10\n",
    "# for i in tqdm(range(max_tests)):\n",
    "#     res = check(gsm8k_dataset[\"train\"][\"answer\"][i], gsm8k_dataset[\"train\"][\"answer\"][i])\n",
    "#     if not res:\n",
    "#         print(gsm8k_dataset[\"train\"][\"answer\"][i])\n",
    "#     total += res\n",
    "\n",
    "# total/max_tests * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_llm(instruction_score_pairs, training_sample):\n",
    "    pairs_str = \"\"\n",
    "    for pair in instruction_score_pairs:\n",
    "        pairs_str += f\"input:\\n{pair[0]}\\nscore:\\n{pair[1]}\\n\"\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "        You are an optimization expert. The user has some texts along with their corresponding scores.\n",
    "        Your task is to generate a new piece of text in square brackets that scores as high as possible.\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = \"\"\"\n",
    "        I have some texts along with their corresponding scores. The texts are arranged in ascending order\n",
    "        based on their scores, where higher scores indicate better quality.\n",
    "\n",
    "        {pairs_str}\n",
    "\n",
    "        The following exemplars show how to apply your text: you replace <INS> in each input with your\n",
    "        text, then read the input and give an output. We say your output is wrong if your output is different\n",
    "        from the given output, and we say your output is correct if they are the same.\n",
    "                \n",
    "        input:\n",
    "        Q: {q1}\n",
    "        A: <INS>\n",
    "        output:\n",
    "        {a1}\n",
    "\n",
    "        input:\n",
    "        Q: {q2}\n",
    "        A: <INS>\n",
    "        output:\n",
    "        {a2}\n",
    "\n",
    "        input:\n",
    "        Q: {q3}\n",
    "        A: <INS>\n",
    "        output:\n",
    "        {a3}\n",
    "        \n",
    "        Write your new text that is different from the old ones and has a score as high as possible. Write the\n",
    "        text in square brackets.\n",
    "    \"\"\"\n",
    "    q1, q2, q3 = training_sample[\"question\"][0], training_sample[\"question\"][1], training_sample[\"question\"][2]\n",
    "    a1, a2, a3 = training_sample[\"answer\"][0], training_sample[\"answer\"][1], training_sample[\"answer\"][2]\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt.format(pairs_str=pairs_str, q1=q1, q2=q2, q3=q3, a1=a1, a2=a2, a3=a3)},\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo-0125\",\n",
    "        max_tokens=4096,\n",
    "        temperature=1,\n",
    "    )\n",
    "    result = chat_completion.choices[0].message.content\n",
    "    return result[1:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer_lm(instruction, training_sample):\n",
    "    accuracy = 0\n",
    "    user_prompt = \"\"\"\n",
    "        Q: {question}\n",
    "        A: {instruction}\n",
    "    \"\"\"\n",
    "    for i in tqdm(range(len(training_sample))):\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_prompt.format(\n",
    "                        question=training_sample[\"question\"][i], instruction=instruction\n",
    "                    ),\n",
    "                },\n",
    "            ],\n",
    "            model=\"gpt-3.5-turbo-0125\",\n",
    "            max_tokens=4096,\n",
    "            temperature=0,\n",
    "        )\n",
    "        result = chat_completion.choices[0].message.content\n",
    "        accuracy += check_fast(result, training_sample[\"answer\"][i])\n",
    "\n",
    "    accuracy = accuracy / len(training_sample) * 100\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the Optimizer with the Scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [01:27<00:00,  3.38s/it]\n",
      "100%|██████████| 26/26 [01:19<00:00,  3.04s/it]\n",
      "100%|██████████| 26/26 [01:35<00:00,  3.67s/it]\n",
      "100%|██████████| 26/26 [01:14<00:00,  2.87s/it]\n",
      "100%|██████████| 26/26 [01:11<00:00,  2.74s/it]\n",
      "100%|██████████| 26/26 [01:22<00:00,  3.16s/it]\n",
      "100%|██████████| 26/26 [01:23<00:00,  3.20s/it]\n",
      "100%|██████████| 26/26 [01:09<00:00,  2.66s/it]\n",
      "100%|██████████| 1/1 [10:53<00:00, 653.50s/it]\n"
     ]
    }
   ],
   "source": [
    "# Sample 3.5% of the training set\n",
    "INS_PER_STEP = 8\n",
    "EXEMPLARS_PER_STEP = 3\n",
    "MAX_INS_SCORE_PAIRS = 20\n",
    "SAMPLE_PERCENTAGE = 0.35/100\n",
    "alt_percentage = 0.05/100\n",
    "training_sample = gsm8k_dataset[\"train\"].shuffle(seed=42).select(range(int(len(gsm8k_dataset[\"train\"]) * SAMPLE_PERCENTAGE)))\n",
    "# ins_score_pairs = {\"Let's think step by step to solve this problem.\": scorer_lm(\"Let's think step by step to solve this problem.\", training_sample)}\n",
    "ins_score_pairs = {}\n",
    "\n",
    "steps = 1\n",
    "for i in tqdm(range(steps)):\n",
    "    # Optimizer LLM\n",
    "    exemplars = training_sample.shuffle()[:EXEMPLARS_PER_STEP]\n",
    "    instructions = [opt_llm(ins_score_pairs, exemplars) for _ in range(INS_PER_STEP)]\n",
    "    \n",
    "    # Scoring the new instructions\n",
    "    new_ins_score_pairs = {ins: scorer_lm(ins, training_sample) for ins in instructions}\n",
    "    ins_score_pairs.update(new_ins_score_pairs)\n",
    "    ins_score_pairs = dict(sorted(ins_score_pairs.items(), key=lambda x: x[1], reverse=True)[:MAX_INS_SCORE_PAIRS])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'By implementing advanced optimization techniques, we can significantly enhance the efficiency and performance of the system, resulting in optimal outcomes.': 88.46153846153845,\n",
       " 'The solution to this problem lies in optimizing the algorithm to efficiently calculate the total score.': 84.61538461538461,\n",
       " 'By perfecting his aim, Thomas achieved a perfect stack of 100 blocks, impressing everyone with his precision and dedication.': 84.61538461538461,\n",
       " 'Based on various scientific studies, experts have concluded that implementing regular exercise routines leads to numerous health benefits, such as improved cardiovascular health, increased energy levels, and enhanced mental well-being.]': 80.76923076923077,\n",
       " 'I calculated the sum of the first 10 prime numbers which are 2, 3, 5, 7, 11, 13, 17, 19, 23, 29. The total sum is 129. This demonstrates the importance of prime numbers in mathematics.': 80.76923076923077,\n",
       " 'To maximize your score, focus on providing clear, concise, and accurate information in your response. Use appropriate language and grammar to convey the message effectively. Ensure that your text directly addresses the question or prompt at hand and avoids unnecessary or irrelevant details. By following these guidelines, you can create a new piece of text that scores highly in terms of quality and relevance.': 80.76923076923077,\n",
       " 'After carefully constructing our solution, we arrived at the most optimum answer.': 76.92307692307693,\n",
       " 'By investing time in continuous learning and improvement, one can truly unlock their full potential and achieve remarkable success.': 76.92307692307693}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ins_score_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
