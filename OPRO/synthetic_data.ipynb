{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Synthetic Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from OPRO import OPRO\n",
    "from datasets import load_dataset, Dataset\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "# load gsm8k dataset\n",
    "gsm8k_dataset = load_dataset(\"gsm8k\", \"main\")\n",
    "\n",
    "opro = OPRO([\"anthropic\", \"gemini\", \"gemma\"])  # [\"gemini\", \"gemma\"]\n",
    "\n",
    "# Generate a question and answer pair using a language model\n",
    "def generate_qa_pair():\n",
    "    prompt_template = \"\"\"You are a helpful assistant designed to generate synthetic questions and answers to those questions similar to the gsm8k dataset.\n",
    "\n",
    "Please generate a question and answer pair that is similar to the following question and answer pair as a JSON object:\n",
    "\n",
    "{{\n",
    "    \"question\": \\\"\\\"\\\"{question}\\\"\\\"\\\",\n",
    "    \"answer\": \\\"\\\"\\\"{answer}\\\"\\\"\\\"\n",
    "}}\n",
    "\n",
    "Make sure the questions and answers are string values.\n",
    "Take a deep breath and think step-by-step.\n",
    "\"\"\"\n",
    "\n",
    "    response = opro.generate(\n",
    "        prompt_template.format(\n",
    "            question=gsm8k_dataset[\"train\"][\"question\"][0],\n",
    "            answer=gsm8k_dataset[\"train\"][\"answer\"][0],\n",
    "        ),\n",
    "        model=\"anthropic\",  # gemini\n",
    "        is_indeterministic=True,\n",
    "    )\n",
    "    return eval(response)\n",
    "\n",
    "\n",
    "if not os.path.exists(\"synthetic_data.json\"):\n",
    "    qa_pairs = []\n",
    "    SAMPLE_SIZE = 250\n",
    "\n",
    "    # Generating synthetic data\n",
    "    pbar = tqdm(total=SAMPLE_SIZE)\n",
    "    while len(qa_pairs) < SAMPLE_SIZE:\n",
    "        try:\n",
    "            qa_pair = generate_qa_pair()\n",
    "            qa_pairs.append(qa_pair)\n",
    "            pbar.update(1)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    pbar.close()\n",
    "\n",
    "    # Saving to file\n",
    "    with open(\"synthetic_data.json\", \"w\") as f:\n",
    "        f.write(str(qa_pairs))\n",
    "else:\n",
    "    # Reading saved data\n",
    "    with open(\"synthetic_data.json\", \"r\") as f:\n",
    "        qa_pairs = eval(f.read())\n",
    "\n",
    "qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_SAMPLE_SIZE = 30\n",
    "TESTING_SAMPLE_SIZE = 50\n",
    "\n",
    "training_sample = Dataset.from_dict(\n",
    "    {\n",
    "        \"question\": [pair[\"question\"] for pair in qa_pairs[:TRAINING_SAMPLE_SIZE]],\n",
    "        \"answer\": [pair[\"answer\"] for pair in qa_pairs[:TRAINING_SAMPLE_SIZE]],\n",
    "    }\n",
    ")\n",
    "\n",
    "testing_sample = Dataset.from_dict(\n",
    "    {\n",
    "        \"question\": [\n",
    "            pair[\"question\"]\n",
    "            for pair in qa_pairs[\n",
    "                TRAINING_SAMPLE_SIZE : TRAINING_SAMPLE_SIZE + TESTING_SAMPLE_SIZE\n",
    "            ]\n",
    "        ],\n",
    "        \"answer\": [\n",
    "            pair[\"answer\"]\n",
    "            for pair in qa_pairs[\n",
    "                TRAINING_SAMPLE_SIZE : TRAINING_SAMPLE_SIZE + TESTING_SAMPLE_SIZE\n",
    "            ]\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Checking if the lists are parallel, i.e. the question and answer at the same index are related\n",
    "len(training_sample[\"question\"]), len(training_sample[\"answer\"]), len(\n",
    "    testing_sample[\"question\"]\n",
    "), len(testing_sample[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize prompt on synthetic data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_llm(instruction_score_pairs, training_sample):\n",
    "    # Format the instruction and score pairs into a string\n",
    "    pairs_str = \"\"\n",
    "    for ins, score in instruction_score_pairs.items():\n",
    "        pairs_str += f\"text:\\n{ins}\\nscore:\\n{score:.2f}\\n\\n\"\n",
    "\n",
    "    prompt = \"\"\"You are an optimization expert. The user has some texts along with their corresponding scores.\n",
    "Your task is to generate a new piece of text that scores as high as possible. \n",
    "Generate the new unique text only, not its corresponding score.\n",
    "\n",
    "I have some texts along with their corresponding scores. The texts are arranged in ascending order\n",
    "based on their scores, where higher scores indicate better quality.\n",
    "\n",
    "{pairs_str}\n",
    "\n",
    "The following exemplars show how to apply your text: you replace <INS> in each input with your\n",
    "text, then read the input and give an output. We say your output is wrong if your output is different\n",
    "from the given output, and we say your output is correct if they are the same.\n",
    "\n",
    "input:\n",
    "Q: {q1}\n",
    "A: <INS>\n",
    "output:\n",
    "{a1}\n",
    "\n",
    "input:\n",
    "Q: {q2}\n",
    "A: <INS>\n",
    "output:\n",
    "{a2}\n",
    "\n",
    "input:\n",
    "Q: {q3}\n",
    "A: <INS>\n",
    "output:\n",
    "{a3}\n",
    "\n",
    "Write your new text that is different from the old ones and has a score as high as possible.\n",
    "Generate the new unique text only, not its corresponding score.\n",
    "New instruction:\n",
    "\"\"\"\n",
    "\n",
    "    # Passing 20 Best Instruction-Score Pairs\n",
    "    # 3 randomly chosen questions/answers from the training sample\n",
    "    cut = lambda x: x.split(\"####\")[-1].strip()\n",
    "    q1, q2, q3 = (\n",
    "        training_sample[\"question\"][0],\n",
    "        training_sample[\"question\"][1],\n",
    "        training_sample[\"question\"][2],\n",
    "    )\n",
    "    a1, a2, a3 = (\n",
    "        cut(training_sample[\"answer\"][0]),\n",
    "        cut(training_sample[\"answer\"][1]),\n",
    "        cut(training_sample[\"answer\"][2]),\n",
    "    )\n",
    "    response = opro.generate(\n",
    "        prompt.format(pairs_str=pairs_str, q1=q1, a1=a1, q2=q2, a2=a2, q3=q3, a3=a3),\n",
    "        model=\"gemini\",  # gemini\n",
    "        is_indeterministic=True\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "def score(instruction, sample):\n",
    "    \"\"\"\n",
    "    Score the instruction using the sample. \n",
    "    \n",
    "    Args:\n",
    "    instruction: str\n",
    "    sample: Dataset with \"question\" and \"answer\" as keys\n",
    "\n",
    "    Returns:\n",
    "    accuracy: float\n",
    "    \"\"\"\n",
    "    accuracy = 0\n",
    "    prompt = \"\"\"Q: {question}\\nA: {instruction}\"\"\"\n",
    "    with tqdm(sample, desc=instruction, position=1, leave=False) as pbar:\n",
    "        for idx, sample_qa in enumerate(pbar):\n",
    "            res = opro.generate(\n",
    "                prompt.format(question=sample_qa[\"question\"], instruction=instruction),\n",
    "                model=\"gemma\",  # gemma\n",
    "            )\n",
    "            # Heuristic for detecting correctness\n",
    "            accuracy += sample_qa[\"answer\"].split(\"####\")[-1].strip() in res\n",
    "            pbar.set_postfix({\"Accuracy\": f\"{accuracy / (idx + 1):.2f}\"})\n",
    "\n",
    "    return accuracy / len(sample) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 3.5% of the training set\n",
    "INS_PER_STEP = 8\n",
    "EXEMPLARS_PER_STEP = 3\n",
    "MAX_INS_SCORE_PAIRS = 20\n",
    "SAVE_PATH = \"synthetic_data_results_k.json\"\n",
    "\n",
    "# loading saved data\n",
    "if os.path.exists(SAVE_PATH):\n",
    "    with open(SAVE_PATH, \"r\") as f:\n",
    "        results = json.load(f)\n",
    "    starting_step = len(results) + 1\n",
    "    ins_score_pairs = results[str(len(results))]\n",
    "else:\n",
    "    ins_lst = [\n",
    "        \"Solve this problem\",\n",
    "        \"Let's think step by step\",\n",
    "        \"Let’s solve this problem by splitting it into steps\",\n",
    "        \"Let’s think about this logically\",\n",
    "        \"Take a deep breath and think through this\",\n",
    "        \"Break this down\",\n",
    "        \"Respond quickly\",\n",
    "        \"a;lsdghougkaa;aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\",\n",
    "    ]\n",
    "    ins_score_pairs = {\n",
    "        ins: score(ins, training_sample)\n",
    "        for ins in tqdm(ins_lst, desc=\"Scoring\", position=0)\n",
    "    }\n",
    "    starting_step = 1\n",
    "    results = {starting_step: ins_score_pairs}\n",
    "    with open(SAVE_PATH, \"w\") as f:\n",
    "        json.dump(results, f)\n",
    "\n",
    "# Each step takes aboy 5 to 10 minutes with gemma:2b\n",
    "STEP_COUNT = 10\n",
    "TARGET_STEP = starting_step + STEP_COUNT\n",
    "assert TARGET_STEP >= starting_step, \"TARGET_STEP should be greater than step.\"\n",
    "for i in range(starting_step, TARGET_STEP):\n",
    "    print(f\"Step {i}\")\n",
    "    while True:\n",
    "        try:\n",
    "            # Optimizer LLM\n",
    "            exemplars = training_sample.shuffle()[:EXEMPLARS_PER_STEP]\n",
    "            instructions = [\n",
    "                opt_llm(ins_score_pairs, exemplars)\n",
    "                for _ in trange(INS_PER_STEP, desc=\"Optimizing\")\n",
    "            ]\n",
    "\n",
    "            # Scoring the new instructions\n",
    "            new_ins_score_pairs = {\n",
    "                ins: score(ins, training_sample)\n",
    "                for ins in tqdm(instructions, desc=\"Scoring\", position=0)\n",
    "            }\n",
    "            combined_ins_score_pairs = {**ins_score_pairs, **new_ins_score_pairs}\n",
    "            ins_score_pairs = dict(\n",
    "                sorted(\n",
    "                    combined_ins_score_pairs.items(), key=lambda x: x[1], reverse=True\n",
    "                )[:MAX_INS_SCORE_PAIRS]\n",
    "            )\n",
    "\n",
    "            # Saving data\n",
    "            results[i] = ins_score_pairs\n",
    "            with open(SAVE_PATH, \"w\") as f:\n",
    "                json.dump(results, f)\n",
    "\n",
    "            break\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins_score_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Results and Testing Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load opro.json\n",
    "with open(SAVE_PATH) as f:\n",
    "    opro_results = json.load(f)\n",
    "\n",
    "# opro = {step:dict(sorted(opro[step].items(), key=lambda x: x[1], reverse=True)[:20]) for step in opro}\n",
    "step_to_accuracy = {\n",
    "    step: (\n",
    "        statistics.mean(opro_results[step].values()),\n",
    "        statistics.stdev(opro_results[step].values()),\n",
    "    )\n",
    "    for step in opro_results\n",
    "}\n",
    "\n",
    "# Plot step to accuracy as a continuous line graph, including stdevs as highlighted error bars\n",
    "fig, ax = plt.subplots()\n",
    "ax.errorbar(\n",
    "    step_to_accuracy.keys(),\n",
    "    [accuracy[0] for accuracy in step_to_accuracy.values()],\n",
    "    yerr=[accuracy[1] for accuracy in step_to_accuracy.values()],\n",
    "    fmt=\"o\",\n",
    ")\n",
    "ax.set_xticks([step for step in step_to_accuracy.keys() if int(step) % 5 == 0])\n",
    "ax.set_xlabel(\"Step\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_title(\"Step to Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opro_results[str(len(opro_results))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_instruction = max(\n",
    "    opro_results[str(len(opro_results))], key=opro_results[str(len(opro_results))].get\n",
    ")\n",
    "print(\"No instruction: \", score(\"\", testing_sample))\n",
    "print(\n",
    "    f\"With best instruction ({best_instruction}): \",\n",
    "    score(best_instruction, testing_sample),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_original = Dataset.from_dict(gsm8k_dataset[\"test\"][:250])\n",
    "best_instruction = max(\n",
    "    opro_results[str(len(opro_results))], key=opro_results[str(len(opro_results))].get\n",
    ")\n",
    "print(\"No instruction: \", score(\"\", data_original))\n",
    "print(\n",
    "    f\"With best instruction ({best_instruction}): \",\n",
    "    score(best_instruction, data_original),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
