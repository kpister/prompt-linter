{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Scoring Mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rouge**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge-1': {'r': 0.7142857142857143, 'p': 0.8333333333333334, 'f': 0.7692307642603551}, 'rouge-2': {'r': 0.5, 'p': 0.6, 'f': 0.5454545404958678}, 'rouge-l': {'r': 0.7142857142857143, 'p': 0.8333333333333334, 'f': 0.7692307642603551}}\n",
      "0.6946386896721926\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "# Example generated and reference texts\n",
    "generated_text = \"The cat sat on the mat.\"\n",
    "reference_text = \"The cat is sitting on the mat.\"\n",
    "\n",
    "# Initialize the Rouge object\n",
    "rouge = Rouge()\n",
    "\n",
    "# Compute the scores\n",
    "scores = rouge.get_scores(generated_text, reference_text)[0]\n",
    "total_score = 0\n",
    "for r in scores:\n",
    "    total_score += scores[r]['f']\n",
    "print(scores)\n",
    "print(total_score/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "def rouge_scorer():\n",
    "    # Initialize the Rouge object\n",
    "    rouge = Rouge()\n",
    "\n",
    "    def compute_scores(generated_text, reference_text):\n",
    "        # Compute the scores\n",
    "        scores = rouge.get_scores(generated_text, reference_text)[0]\n",
    "        total_score = 0\n",
    "        for r in scores:\n",
    "            total_score += scores[r]['f']\n",
    "        return total_score/3\n",
    "\n",
    "    return compute_scores\n",
    "\n",
    "# Usage\n",
    "score_rouge = rouge_scorer()\n",
    "avg_score = score_rouge(\"The cat sat on the mat.\", \"The cat is sitting on the mat.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLEU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4548019047027907\n",
      "0.4548019047027907\n",
      "0.4548019047027907\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# Example generated and reference texts\n",
    "reference_texts = [[\"The cat is sitting on the mat.\"]]\n",
    "generated_text = \"The cat sat on the mat.\"\n",
    "generated_text = ['It', 'is', 'a', 'cat', 'at', 'room']\n",
    "reference_texts = ['It', 'is', 'a', 'cat', 'inside', 'the', 'room']\n",
    "\n",
    "# Tokenize the texts\n",
    "reference_tokens = [reference_texts]\n",
    "generated_tokens = generated_text\n",
    "\n",
    "# Compute the BLEU score\n",
    "bleu_score = sentence_bleu(reference_tokens, generated_tokens)\n",
    "\n",
    "print(bleu_score)\n",
    "\n",
    "def bleu_score(expected, actual):\n",
    "    return sentence_bleu([expected.split()], actual.split())\n",
    "\n",
    "print(bleu_score(\"It is a cat inside the room.\", \"It is a cat at room.\"))\n",
    "\n",
    "bleu_score = lambda expected, actual: sentence_bleu([expected.split()], actual.split())\n",
    "print(bleu_score(\"It is a cat inside the room.\", \"It is a cat at room.\"))\n",
    "\n",
    "# Disable the warning message for sentence_blue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**METEOR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "# Example generated and reference texts\n",
    "reference_texts = [[\"The cat is sitting on the mat.\"]]\n",
    "generated_text = [\"The cat is sitting on the mat.\"]\n",
    "\n",
    "# Compute the METEOR score\n",
    "# apparently it's better alternative compared to BLEU and ROUGE as it \n",
    "# incorporates recall, precision, and additional semantic matching based on stems and paraphrasing )\n",
    "# idk how it works... will read more about it later\n",
    "meteor_score = meteor_score(reference_texts, generated_text)\n",
    "\n",
    "print(meteor_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BERTScore**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26085e506f884b8a99d3c4990d85704e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae8a0d0e3df48e5b062445eced0dd78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c59477171df14291940f61ab5e283002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a5d6af8fe3a4acb9edfa200aeb6830a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973e4e3b17d649669412907e1b40e1c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77af69b6356e4e92a8af7f49b48fb30c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f34c2f04cd94fe39b2202dfdcfe4369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742652ca8afe4459a7c8eeb9c44216f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.35 seconds, 2.88 sentences/sec\n",
      "Precision: 0.9754238724708557, Recall: 0.965898334980011, F1: 0.9706377387046814\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "\n",
    "# Example generated and reference texts\n",
    "reference_texts = [\"The cat is sitting on the mat.\"]\n",
    "generated_text = [\"The cat sat on the mat.\"]\n",
    "\n",
    "# Compute the BERTScore\n",
    "P, R, F1 = score(generated_text, reference_texts, lang='en', verbose=True)\n",
    "\n",
    "print(f\"Precision: {P.mean().item()}, Recall: {R.mean().item()}, F1: {F1.mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
